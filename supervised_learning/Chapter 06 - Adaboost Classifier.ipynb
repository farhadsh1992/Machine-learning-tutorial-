{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Chapter 6: Adaboost Classifier</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Refrance:</h2><br>\n",
    "<b>Theory and coding:</b><br>\n",
    "\n",
    "https://medium.com/machine-learning-101/https-medium-com-savanpatel-chapter-6-adaboost-classifier-b945f330af06\n",
    "<br><b>gitHub:</b><br>\n",
    "\n",
    "<br><b>orginal website:</b><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>What does Ada-boost classifier do?</b><br>\n",
    "Ada-boost, like Random Forest Classifier is another ensemble classifier. <br>\n",
    "(Ensemble classifier are made up of multiple classifier algorithms and whose output is combined result of output of those classifier algorithms).<br>\n",
    "Ada-boost classifier combines weak classifier algorithm to form strong classifier. <br>\n",
    "A single algorithm may classify the objects poorly. But if we combine multiple classifiers with selection of training set at every iteration and assigning right amount of weight in final voting, we can have good accuracy score for overall classifier.<br>\n",
    "\n",
    "<b>In short Ada-boost:</b><br>\n",
    "1. retrains the algorithm iteratively by choosing the training set based on accuracy of previous training.\n",
    "2. The weight-age of each trained classifier at any iteration depends on the accuracy achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>How do we select the training set?</b><br>\n",
    "Each weak classifier is trained using a random subset of overall training set.<br>\n",
    "\n",
    "Each weak classifier is trained using a random subset of overall training set.<br>\n",
    "After each classifier is trained, the weight is assigned to the classifier as well based on accuracy.<br>\n",
    "More accurate classifier is assigned higher weight so that it will have more impact in final outcome.<br>\n",
    "\n",
    "<b>How to assign weight to each classifier?</b><br>\n",
    "A classifier with 50% accuracy is given a weight of zero, and a classifier with less than 50% accuracy is given negative weight.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Mathematics</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the mathematical formula and parameters:<br>\n",
    "H(x) = sign(∑ ( a_t * h_t(x))<br>\n",
    "\n",
    "h_t(x) is the output of weak classifier t for input x<br>\n",
    "alpha_t is weight assigned to classifier. alpha_t is calculated as follows:<br>\n",
    "alpha_t = 0.5 * ln( (1 — E)/E) <br>\n",
    "\n",
    "weight of classifier is straigt forward, it is based on the error rate E.<br>\n",
    "Initially, all the input training example has equal weightage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>A plot of alpha_t v/s error rate:</h3>\n",
    "https://medium.com/machine-learning-101/https-medium-com-savanpatel-chapter-6-adaboost-classifier-b945f330af06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating weight of training examples\n",
    "After weak classifier is trained, we update the weight of each training example with following formula,<br>\n",
    "\n",
    "<br>\n",
    "D_t is weight at previous level.<br>\n",
    "We normalize the weights by dividing each of them by the sum of all the weights, Z_t. For example, if all of the calculated weights added up to 15.7, then we would divide each of the weights by 15.7 so that they sum up to 1.0 instead.<br>\n",
    "y_i is y par of training example (x_i, y_i) y coordinate for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Final Thoughts</b><br>\n",
    "Adaboost like random forest classifier gives more accurate results since it depends upon many weak classifier for final decision. One of the applications to Adaboost is for face recognition systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types of learners in classification as lazy learners and eager learners.\n",
    "1. Lazy learners<br>\n",
    "Lazy learners simply store the training data and wait until a testing data appear. When it does, classification is conducted based on the most related data in the stored training data. Compared to eager learners, lazy learners have less training time but more time in predicting.\n",
    "Ex. k-nearest neighbor, Case-based reasoning\n",
    "2. Eager learners<br>\n",
    "Eager learners construct a classification model based on the given training data before receiving data for classification. It must be able to commit to a single hypothesis that covers the entire instance space. Due to the model construction, eager learners take a long time for train and less time to predict.\n",
    "Ex. Decision Tree, Naive Bayes, Artificial Neural Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Evaluating a classifier</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Refrance:</h2>\n",
    "https://towardsdatascience.com/machine-learning-classifiers-a5cc4e1b0623"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Holdout method:</b><br>\n",
    "There are several methods exists and the most common method is the holdout method. In this method, the given data set is divided into 2 partitions as test and train 20% and 80% respectively. The train set will be used to train the model and the unseen test data will be used to test its predictive power.<br>\n",
    "\n",
    "<b>Cross-validation:</b><br>\n",
    "Over-fitting is a common problem in machine learning which can occur in most models. k-fold cross-validation can be conducted to verify that the model is not over-fitted. In this method, the data-set is randomly partitioned into k mutually exclusive subsets, each approximately equal size and one is kept for testing while others are used for training. This process is iterated throughout the whole k folds.<br>\n",
    "\n",
    "<b>Precision and Recall:</b><br>\n",
    "Precision is the fraction of relevant instances among the retrieved instances, while recall is the fraction of relevant instances that have been retrieved over the total amount of relevant instances. Precision and Recall are used as a measurement of the relevance.<br>\n",
    "\n",
    "<b>ROC curve ( Receiver Operating Characteristics):</b><br>\n",
    "ROC curve is used for visual comparison of classification models which shows the trade-off between the true positive rate and the false positive rate. The area under the ROC curve is a measure of the accuracy of the model. When a model is closer to the diagonal, it is less accurate and the model with perfect accuracy will have an area of 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

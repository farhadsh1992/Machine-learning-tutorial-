{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Naive Bayes Classifier</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Refrance:</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/machine-learning-101/chapter-1-supervised-learning-and-naive-bayes-classification-part-1-theory-8b9e361897d5\n",
    "\n",
    "https://medium.com/machine-learning-101/chapter-1-supervised-learning-and-naive-bayes-classification-part-2-coding-5966f25f1475\n",
    "\n",
    "https://medium.com/machine-learning-101\n",
    "\n",
    "https://github.com/savanpatel/machine-learning-101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes classifier calculates the probabilities for every factor ( here in case of email example would be Alice and Bob for given input feature). Then it selects the outcome with highest probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This classifier assumes the features (in this case we had words as input) are independent. Hence the word naive. Even with this it is powerful algorithm used for:\n",
    "- Real time Prediction\n",
    "- Text classification/ Spam Filtering\n",
    "- Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a certain event E and test actors x1,x2,x3, etc.\n",
    "We first calculate P(x1| E) , P(x2 | E) … [read as probability of x1 given event E happened] and then select the test actor x with maximum probability value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Clean Data</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use sklearn in Python and implement Naive Bayes classifier for labelling email to either as Spam or Ham:\n",
    "\n",
    "In this exercise we shall train the model with set of emails labelled as either from Spam or Not Spam. There are 702 emails equally divided into spam and non spam category. Next, we shall test the model on 260 emails.<br> \n",
    "https://github.com/savanpatel/machine-learning-101/blob/master/chapter1/code/classifier.py\n",
    "\n",
    "\n",
    "make_Dictionary reads the email files from a folder, constructs a dictionary for all words. Next, we delete words of length 1 and that are not purely alphabetical.\n",
    "At last we only extract out 3000 most common words.\n",
    "word frequency::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_Dictionary(root_dir):\n",
    "    all_words = []\n",
    "    emails = [os.path.join(root_dir,f) for f in os.listdir(root_dir)]\n",
    "    for mail in emails:\n",
    "        with open(mail) as m:\n",
    "            for line in m:\n",
    "                words = line.split()\n",
    "                all_words += words\n",
    "    dictionary = Counter(all_words)\n",
    "    # if you have python version 3.x use commented version.\n",
    "    # list_to_remove = list(dictionary)\n",
    "    list_to_remove = dictionary.keys()\n",
    "    for item in list_to_remove:\n",
    "        # remove if numerical. \n",
    "        if item.isalpha() == False:\n",
    "            del dictionary[item]\n",
    "        elif len(item) == 1:\n",
    "            del dictionary[item]\n",
    "    # consider only most 3000 common words in dictionary.\n",
    "    dictionary = dictionary.most_common(3000)\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next with the help of dictionary, we generate a label and word frequency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(mail_dir):\n",
    "  files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]\n",
    "  features_matrix = np.zeros((len(files),3000))\n",
    "  train_labels = np.zeros(len(files))\n",
    "  count = 0;\n",
    "  docID = 0;\n",
    "  for fil in files:\n",
    "    with open(fil) as fi:\n",
    "      for i,line in enumerate(fi):\n",
    "        if i == 2:\n",
    "          words = line.split()\n",
    "          for word in words:\n",
    "            wordID = 0\n",
    "            for i,d in enumerate(dictionary):\n",
    "              if d[0] == word:\n",
    "                wordID = i\n",
    "                features_matrix[docID,wordID] = words.count(word)\n",
    "      train_labels[docID] = 0;\n",
    "      filepathTokens = fil.split('/')\n",
    "      lastToken = filepathTokens[len(filepathTokens) - 1]\n",
    "      if lastToken.startswith(\"spmsg\"):\n",
    "          train_labels[docID] = 1;\n",
    "          count = count + 1\n",
    "      docID = docID + 1\n",
    "  return features_matrix, train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training and predicting with sklearn Naive Bayes</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, sklearn Naive Bayes provides three alternatives for model training:\n",
    "- <b>Gaussian:</b> It is used in classification and it assumes that features follow a normal distribution.\n",
    "- <b>Multinomial:</b> It is used for discrete counts. For example, let’s say, we have a text classification problem. Here we can consider bernoulli trials which is one step further and instead of “word occurring in the document”, we have “count how often word occurs in the document”, you can think of it as “number of times outcome number x_i is observed over the n trials”.\n",
    "- <b>Bernoulli:</b> The binomial model is useful if your feature vectors are binary (i.e. zeros and ones). One application would be text classification with ‘bag of words’ model where the 1s & 0s are “word occurs in the document” and “word does not occur in the document” respectively.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/naive_bayes.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we shall use Gaussian. The sample code snippet looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = \"../train-mails\"\n",
    "TEST_DIR = \"../test-mails\"\n",
    "dictionary = make_Dictionary(TRAIN_DIR)\n",
    "\n",
    "\n",
    "# using functions mentioned above.\n",
    "features_matrix, labels = extract_features(TRAIN_DIR)\n",
    "test_feature_matrix, test_labels = extract_features(TEST_DIR)\n",
    "\n",
    "model = GaussianNB()\n",
    "#train model\n",
    "model.fit(features_matrix, labels)\n",
    "#predict\n",
    "predicted_labels = model.predict(test_feature_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Accuracy Score</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(test_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Naive Bayes considers the independence in features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training exmaple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Multinomial Naive Bayes Classifier for Text Analysis (Python)</b><br>\n",
    "https://towardsdatascience.com/multinomial-naive-bayes-classifier-for-text-analysis-python-8dd6825ece67\n",
    "\n",
    "<br><b>Building A SPAM Detector with Naïve Bayes and AdaBoost Machine Learning Classifiers in JupyterLab.</b><br>\n",
    "https://medium.com/datadriveninvestor/building-a-spam-detector-with-naïve-bayes-and-adaboost-machine-learning-classifiers-in-jupyterlab-d99d35f4d066\n",
    "<br><b>Implementing a Multinomial Naive Bayes Classifier from Scratch with Python</b><br>\n",
    "https://medium.com/@johnm.kovachi/implementing-a-multinomial-naive-bayes-classifier-from-scratch-with-python-e70de6a3b92e<br>\n",
    "make all of model and don't use sklearn.\n",
    "<br><b></b><br>\n",
    "\n",
    "<br><b></b><br>\n",
    "\n",
    "<br><b></b><br>\n",
    "\n",
    "<br><b></b><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

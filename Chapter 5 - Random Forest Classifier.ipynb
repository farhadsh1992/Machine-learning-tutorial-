{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Chapter 5: Random Forest Classifier</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Refrance:</h2><br>\n",
    "<b>Theory and coding:</b><br>\n",
    "\n",
    "https://medium.com/machine-learning-101/chapter-5-random-forest-classifier-56dc7425c3e1\n",
    "\n",
    "<br><b>gitHub:</b><br>\n",
    "\n",
    "<br><b>orginal website:</b><br>\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classifier is ensemble algorithm. <br>\n",
    "<b>Ensembled algorithms</b> are those which combines more than one algorithms of same or different kind for classifying objects. For example, running prediction over Naive Bayes, SVM and Decision Tree and then taking vote for final consideration of class for test object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Random Forest Classifier:</b><br>\n",
    "Random forest classifier creates a set of decision trees from randomly selected subset of training set. It then aggregates the votes from different decision trees to decide the final class of the test object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>In Laymen’s term:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose training set is given as : [X1, X2, X3, X4] with corresponding labels as [L1, L2, L3, L4], random forest may create three decision trees taking input of subset for example,<br>\n",
    "1.[X1, X2, X3]<br>\n",
    "2.[X1, X2, X4]<br>\n",
    "3.[X2, X3, X4] <br>\n",
    "So finally, it predicts based on the majority of votes from each of the decision trees made.<br>\n",
    ">This works well because a single decision tree may be prone to a noise, but aggregate of many decision trees reduce the effect of noise giving more accurate results.\n",
    "\n",
    "<b>The subsets in different decision trees created may overlap</b><br>\n",
    "\n",
    "Alternatively, the random forest can apply weight concept for considering the impact of result from any decision tree. Tree with high error rate are given low weight value and vise versa. This would increase the decision impact of trees with low error rate.<br>\n",
    "<b>Basic Parameters:</b><br>\n",
    "Basic parameters to Random Forest Classifier can be total number of trees to be generated and decision tree related parameters like minimum split, split criteria etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Random Forest and Sklearn in Python (Coding example)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have created a git repository for the data set and the sample code. You can download it from here (Use chapter 5 folder). Its same data set discussed in this chapter. I would suggest you to follow through the discussion and do the coding yourself. In case it fails, you can use/refer my version to understand working."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cleaning data:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning involves removal of stop words, extracting most common words from text etc. In the code example concerned we perform following steps:<br>\n",
    "1. Build dictionary of words from email documents from training set.\n",
    "2. Consider the most common 3000 words.\n",
    "3. For each document in training set, create a frequency matrix for these words in dictionary and corresponding labels. [spam email file names start with prefix “spmsg”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_Dictionary(root_dir):\n",
    "   all_words = []\n",
    "   emails = [os.path.join(root_dir,f) for f in os.listdir(root_dir)]\n",
    "   for mail in emails:\n",
    "        with open(mail) as m:\n",
    "            for line in m:\n",
    "                words = line.split()\n",
    "                all_words += words\n",
    "   dictionary = Counter(all_words)\n",
    "# if you have python version 3.x use commented version.\n",
    "   # list_to_remove = list(dictionary)\n",
    "   list_to_remove = dictionary.keys()\n",
    "for item in list_to_remove:\n",
    "       # remove if numerical. \n",
    "       if item.isalpha() == False:\n",
    "            del dictionary[item]\n",
    "        elif len(item) == 1:\n",
    "            del dictionary[item]\n",
    "    # consider only most 3000 common words in dictionary.\n",
    "dictionary = dictionary.most_common(3000)\n",
    "return dictionary\n",
    "def extract_features(mail_dir):\n",
    "  files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]\n",
    "  features_matrix = np.zeros((len(files),3000))\n",
    "  train_labels = np.zeros(len(files))\n",
    "  count = 0;\n",
    "  docID = 0;\n",
    "  for fil in files:\n",
    "    with open(fil) as fi:\n",
    "      for i,line in enumerate(fi):\n",
    "        if i == 2:\n",
    "          words = line.split()\n",
    "          for word in words:\n",
    "            wordID = 0\n",
    "            for i,d in enumerate(dictionary):\n",
    "              if d[0] == word:\n",
    "                wordID = i\n",
    "                features_matrix[docID,wordID] = words.count(word)\n",
    "      train_labels[docID] = 0;\n",
    "      filepathTokens = fil.split('/')\n",
    "      lastToken = filepathTokens[len(filepathTokens) - 1]\n",
    "      if lastToken.startswith(\"spmsg\"):\n",
    "          train_labels[docID] = 1;\n",
    "          count = count + 1\n",
    "      docID = docID + 1\n",
    "  return features_matrix, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Using Random Forest Classifier:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for using Random Forest Classifier is similar to previous classifiers.\n",
    "1. Import library\n",
    "2. Create model\n",
    "3. Train\n",
    "4. Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = \"../train-mails\"\n",
    "TEST_DIR = \"../test-mails\"\n",
    "\n",
    "dictionary = make_Dictionary(TRAIN_DIR)\n",
    "print \"reading and processing emails from file.\"\n",
    "features_matrix, labels = extract_features(TRAIN_DIR)\n",
    "test_feature_matrix, test_labels = extract_features(TEST_DIR)\n",
    "model = RandomForestClassifier()\n",
    "print \"Training model.\"\n",
    "#train model\n",
    "model.fit(features_matrix, labels)\n",
    "predicted_labels = model.predict(test_feature_matrix)\n",
    "print \"FINISHED classifying. accuracy score : \"\n",
    "print accuracy_score(test_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Parameters:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets understand and play with some of the tuning parameters:<br>\n",
    "<b>n_estimators :</b> Number of trees in forest. Default is 10.<br>\n",
    "<b>criterion:</b> “gini” or “entropy” same as decision tree classifier.<br>\n",
    "<b>min_samples_split:</b> minimum number of working set size at node required to split. Default is 2.<br>\n",
    "You can view full list of tunable parameters on orginal site\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

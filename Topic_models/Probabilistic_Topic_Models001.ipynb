{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:100px;text-align:center;border: 4px solid #3550B7;background-color:#3590B7;color:white\">\n",
    "\n",
    "<header style=\"width:100%;height:100px;\">\n",
    "  <h1><b> Session 001</b></h1>\n",
    "    <h4> Probabilistic_Topic_Models </h4>\n",
    "</header>\n",
    "\n",
    "<div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='border: 4px solid #3590B7;padding:9px;'>\n",
    "\n",
    "By: Farhad Shadmand \n",
    "    \n",
    "https://github.com/farhadsh1992\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# contents:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"position: relative;height:100px;\">\n",
    "    \n",
    "<div style=\"width:250px;position:absolute;left: auto;border:4px;border: 4px solid #3590B7;background-color:#3590B7;color:white\">\n",
    "    <header></header>\n",
    "    <a href=\"#1\" style=\"padding:5px;color:white;text-align: center;\" href=\"#Chapter3\">\n",
    "      <h5 ><b> what is Topic Model?  </b></h5>\n",
    "      </a>\n",
    " </div>\n",
    "    \n",
    " <div style=\"width:310px;position:absolute;left: 255px;border:4px;border: 4px solid #3590B7;background-color:#3590B7;color:white\">\n",
    "    <header></header>\n",
    "    <a href=\"#2\"style=\"position: relative;padding:5px;color:white;text-align: center;\" href=\"#Chapter3\">\n",
    "      <h5 ><b> what is Latent Dirichlet Allocation? </b></h5>\n",
    "      </a>\n",
    "     \n",
    "  </div>\n",
    "    <div style=\"width:500px;position:absolute;left: 565px;border:4px;border: 4px solid #3590B7;background-color:#3590B7;color:white\">\n",
    "    <header></header>\n",
    "    <a href=\"#3\"style=\"position: relative;padding:5px;color:white;text-align: center;\" href=\"#Chapter3\">\n",
    "      <h5 ><b> How does topic coherence score in LDA intuitively makes sense ? </b></h5>\n",
    "      </a>\n",
    "        </div>\n",
    "\n",
    "  \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"position: relative;height:100px;\">\n",
    "    \n",
    "<div style=\"width:250px;position:absolute;left: auto;border:4px;border: 4px solid #3550B7;background-color:#3590B7;color:white\">\n",
    "    <header></header>\n",
    "    <a href=\"#Forward_propagation\" style=\"padding:5px;color:white;text-align: center;\" >\n",
    "      <h4 ><b> python code: </b></h4>\n",
    "      </a>\n",
    " </div>\n",
    "    \n",
    " <div style=\"width:300px;position:absolute;left: 255px;border:4px;border: 4px solid #3550B7;background-color:#3590B7;color:white\">\n",
    "    <header></header>\n",
    "    <a href=\"#backward_propagation\"style=\"position: relative;padding:5px;color:white;text-align: center;\" >\n",
    "      <h4 ><b> Load data </b></h4>\n",
    "      </a>\n",
    "     \n",
    "  </div>\n",
    "    <div style=\"width:300px;position:absolute;left: 555px;border:4px;border: 4px solid #3550B7;background-color:#3590B7;color:white\">\n",
    "    <header></header>\n",
    "    <a href=\"#list\"style=\"position: relative;padding:5px;color:white;text-align: center;\" >\n",
    "      <h4 ><b> Clean data </b></h4>\n",
    "      </a>\n",
    "        </div>\n",
    "\n",
    "    \n",
    "   <div style=\"width:250px;position:absolute;left: 815px;border:4px;border: 4px solid #3550B7;background-color:#3590B7;color:white\">\n",
    "    <header></header>\n",
    "    <a href=\"#list\"style=\"position: relative;padding:5px;color:white;text-align: center;\" >\n",
    "      <h4 ><b> </b></h4>\n",
    "      </a>\n",
    "    </div>\n",
    "  \n",
    "    \n",
    "   <div style=\"width:300px;position:absolute;left: 1070px;border:4px;border: 4px solid #3590B7;background-color:#3590B7;color:white\">\n",
    "    <header></header>\n",
    "    <a href=\"#list\"style=\"position: relative;padding:5px;color:white;text-align: center;\" >\n",
    "      <h4 ><b>  </b></h4>\n",
    "      </a>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"position: relative;height:100px;\">\n",
    "    \n",
    "<div style=\"width:250px;position:absolute;left: auto;border:4px;border: 4px solid #3590B7;background-color:#3590B7;color:white\">\n",
    "    <header></header>\n",
    "    <a href=\"#Forward_propagation\" style=\"padding:5px;color:white;text-align: center;\" href=\"#Chapter3\">\n",
    "      <h4 ><b> gensim.models.Doc2Vec </b></h4>\n",
    "      </a>\n",
    " </div>\n",
    "    \n",
    " <div style=\"width:200px;position:absolute;left: 255px;border:4px;border: 4px solid #3590B7;background-color:#3590B7;color:white\">\n",
    "    <header></header>\n",
    "    <a href=\"#backward_propagation\"style=\"position: relative;padding:5px;color:white;text-align: center;\" href=\"#Chapter3\">\n",
    "      <h4 ><b> text Embedding </b></h4>\n",
    "      </a>\n",
    "     \n",
    "  </div>\n",
    "    <div style=\"width:200px;position:absolute;left: 455px;border:4px;border: 4px solid #3590B7;background-color:#3590B7;color:white\">\n",
    "    <header></header>\n",
    "    <a href=\"#list\"style=\"position: relative;padding:5px;color:white;text-align: center;\" href=\"#Chapter3\">\n",
    "      <h4 ><b> sklearn  </b></h4>\n",
    "      </a>\n",
    "        </div>\n",
    "\n",
    "    \n",
    "   <div style=\"width:450px;position:absolute;left: 615px;border:4px;border: 4px solid #3590B7;background-color:#3590B7;color:white\">\n",
    "    <header></header>\n",
    "    <a href=\"#list\"style=\"position: relative;padding:5px;color:white;text-align: center;\" href=\"#Chapter3\">\n",
    "      <h4 ><b> Plotting words and documents in 2D with SVD </b></h4>\n",
    "      </a>\n",
    "    </div>\n",
    "  \n",
    "    \n",
    "   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:70px;border: 4px solid #3550B7;background-color:#3590B7;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h1> Topic Model and credit card complaint root <h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #3550B7;background-color:#BFE6FF;color:black;border-radius: 5px;padding:7px\">\n",
    "  <strong> Refrence: </strong><br>\n",
    "\n",
    "\n",
    "pyData: https://www.youtube.com/watch?v=_R66X_udxZQ\n",
    "\n",
    "Github: https://github.com/tdhopper/pydata-nyc-2015\n",
    "    \n",
    "medium.com: https://medium.com/mlreview/topic-modeling-with-scikit-learn-e80d33668730\n",
    "\n",
    "medium.com: https://medium.com/@benzgreer/sklearn-lda-vs-gensim-lda-691a9f2e9ab7    \n",
    "\n",
    "    \n",
    "by David Blei, Princeton University (highŸÄlevel): https://www.youtube.com/watch?v=7BMsuyBPx90\n",
    "    \n",
    "    \n",
    "the best (for start): https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/\n",
    "    \n",
    "\n",
    "the best (for start): https://nlpforhackers.io/topic-modeling/\n",
    "\n",
    "    \n",
    "Sklearn LDA vs. GenSim LDA: https://medium.com/@benzgreer/sklearn-lda-vs-gensim-lda-691a9f2e9ab7\n",
    "    \n",
    "    \n",
    "credit card: https://towardsdatascience.com/https-medium-com-vishalmorde-humanizing-customer-complaints-using-nlp-algorithms-64a820cef373\n",
    "    \n",
    "    \n",
    "coherence: https://stats.stackexchange.com/questions/375062/how-does-topic-coherence-score-in-lda-intuitively-makes-sense\n",
    "    \n",
    "Andrius Knispelis: https://www.youtube.com/watch?v=3mHy4OSyRf0&list=RDQMlyJh3a_LBaU&start_radio=1\n",
    "  \n",
    "    \n",
    "Prof. David Blei (highŸÄlevel): https://www.youtube.com/watch?v=FkckgwMHP2s\n",
    "    \n",
    "(highŸÄlevel): https://www.youtube.com/watch?v=_R66X_udxZQ&t=778s\n",
    "    \n",
    "    \n",
    "https://github.com/trinker/topicmodels_learning\n",
    "    \n",
    "    \n",
    "https://towardsdatascience.com/the-complete-guide-for-topics-extraction-in-python-a6aaa6cedbbc\n",
    "    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:70px;border: 4px solid #3550B7;background-color:#3590B7;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h1> what is Topic Model? <h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://www.depends-on-the-definition.com/wp-content/uploads/2018/11/IntroToLDA.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is trying to extract what topics a collection of documents are talking about using some sort of statistocal method.\n",
    "summarsing a documents base or collection of documents bases the topics that are discussed within those documents  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several algorithms for doing topic modeling. The most popular ones include:\n",
    "1. **LDA ‚Äì** Latent Dirichlet Allocation ‚Äì The one we‚Äôll be focusing in this tutorial. Its foundations are Probabilistic Graphical Models\n",
    "2. **LSA or LSI ‚Äì** Latent Semantic Analysis or Latent Semantic Indexing ‚Äì Uses Singular Value Decomposition (SVD) on the Document-Term Matrix. Based on Linear Algebra.\n",
    "3. **NMF ‚Äì** Non-Negative Matrix Factorization ‚Äì Based on Linear Algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"2\"style=\"width:100%;height:70px;border: 4px solid #3550B7;background-color:#3590B7;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h1> what is Latent Dirichlet Allocation? <h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA is a **three_level hierarchal Bayesian model**, in which each time of a collection is modeled as a finite misxture over an underlying set of topics. <br>\n",
    "Each topic is, in turn, modeled as an infinite mixture over an underlying set of topics problities. <br>\n",
    "\n",
    "<font size='6'> $p(D|\\alpha,\\beta) = \\int p(\\theta_d|\\alpha)(\\sqcap^{N_d}_{n=1}\\Sigma_{zd_n} p(z_dn|\\theta_d) p(w_d n| z_d n, \\beta)) $ </font>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='3' style=\"width:100%;height:70px;border: 4px solid #3550B7;background-color:#3590B7;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h1> How does topic coherence score in LDA intuitively makes sense ? <h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://cdn-images-1.medium.com/max/1600/0*II7wZlKViCt4ssBm.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='6'> $ ùê∂ùëú‚ÑéùëíùëüùëíùëõùëêùëíùëÜùëêùëúùëüùëí = \\Sigma_{j<l} score(w_i,w_j)$ </font>\n",
    "\n",
    "where ùë§ùëñ,ùë§ùëó are the top words of the topic. There are two types of topic coherence scores:<br>\n",
    "\n",
    "1. Extrinsic UCI measure:\n",
    "\n",
    "<font size='6'> $ ùëÜùê∂ùëÇùëÖùê∏_{ùëàùê∂ùêº}(ùë§_ùëñ,ùë§_ùëó) = log \\frac{p(ùë§_ùëñ,ùë§_ùëó)}{p(ùë§_ùëñ)p(ùë§_ùëó)} $  </font>\n",
    "\n",
    "where:\n",
    "\n",
    "<font size='6'> $ p(ùë§_ùëñ) = \\frac{D_{wikipedia}(ùë§_ùëñ)}{D_{wikipedia}}  $  </font> <br>\n",
    "and<br>\n",
    "<font size='6'> $ p(ùë§_ùëñ,ùë§_ùëó) = \\frac{D_{wikipedia}(ùë§_ùëñ,ùë§_ùëó)}{D_{wikipedia}}  $  </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Intrinsic UMass measure:\n",
    "\n",
    "<font size='6'> $ ùëÜùê∂ùëÇùëÖùê∏_{ùëàùê∂ùêº}(ùë§_ùëñ,ùë§_ùëó) = \\frac{D_{wikipedia}(ùë§_ùëñ,ùë§_ùëó)+1}{D_{wikipedia}} $ </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coherence score is for assessing the quality of the learned topics. <br>\n",
    "For one topic,the words $ ùëñ,ùëó $ Being scored in $ \\Sigma_{ùëñ<ùëó} Score(ùë§_ùëñ,ùë§_ùëó)$ have the highest probability of occurring for that topic. You need to specify how many words in the topic to consider for the overall score. <br>\n",
    "For the \"UMass\" measure, the numerator $ ùê∑(ùë§_ùëñ,ùë§_ùëó) $ is the number of documents in which words $ùë§_ùëñ$  and $ùë§_ùëó$  appear together. 1 is added to this term because we are taking logs and we need to avoid taking log of 0 when the two words never appear together. The denominator is the number of documents $ùê∑(ùë§_ùëñ)$ appears in. So the score is higher if $ùë§_ùëñ$ and $ùë§_ùëó$ appear together in documents a lot relative to how often $ùë§_ùëñ$  alone appears in documents. This makes sense as a measure of topic coherence, since if two words in a topic really belong together you would expect them to show up together a lot. The denominator is just adjusting for the document frequency of the words you are considering, so that words like \"the\" don't get an artificially high score. <br>\n",
    "You could use the topic coherence scores, $ùê∂ùëÜ(ùë°)$ , to determine the optimal number $ùêæ_‚àó$ of topics by finding $ arg$$max_ùêæ \\frac{1}{ùêæ}\\Sigma^{ùêæ}_{ùë°=1}ùê∂ùëÜ(ùë°)$ . That is take the average topic coherence score for various settings of $ùêæ$ and see which gives the highest average coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"4\" style=\"width:100%;height:70px;border: 6px solid #3590B7;background-color:#3550B7;color:white;text-align:center;padding:3px\">\n",
    "    <h1> python code: <h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 272657 entries, 0 to 272656\n",
      "Data columns (total 14 columns):\n",
      "Date_received                   272657 non-null object\n",
      "Product                         272657 non-null object\n",
      "Sub_product                     220474 non-null object\n",
      "Issue                           272657 non-null object\n",
      "Sub_issue                       174509 non-null object\n",
      "Consumer_complaint_narrative    272657 non-null object\n",
      "Company_public_response         132957 non-null object\n",
      "Company                         272657 non-null object\n",
      "State                           271627 non-null object\n",
      "ZIP_code                        270301 non-null object\n",
      "Submitted_via                   272657 non-null object\n",
      "Date_sent_to_company            272657 non-null object\n",
      "Company_response_to_consumer    272656 non-null object\n",
      "Complaint_ID                    272657 non-null int64\n",
      "dtypes: int64(1), object(13)\n",
      "memory usage: 29.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df_data = pd.read_csv('/Users/apple/Documents/Programming/python/CloserInterview/Closer_Challenge/Exercise/complains_data.csv')\n",
    "df_data.columns =['Date_received','Product', 'Sub_product', 'Issue', 'Sub_issue',\n",
    "       'Consumer_complaint_narrative', 'Company_public_response', 'Company',\n",
    "       'State', 'ZIP_code', 'Submitted_via', 'Date_sent_to_company','Company_response_to_consumer',\n",
    "       'Complaint_ID']\n",
    "df_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date_received</th>\n",
       "      <th>Product</th>\n",
       "      <th>Sub_product</th>\n",
       "      <th>Issue</th>\n",
       "      <th>Sub_issue</th>\n",
       "      <th>Consumer_complaint_narrative</th>\n",
       "      <th>Company_public_response</th>\n",
       "      <th>Company</th>\n",
       "      <th>State</th>\n",
       "      <th>ZIP_code</th>\n",
       "      <th>Submitted_via</th>\n",
       "      <th>Date_sent_to_company</th>\n",
       "      <th>Company_response_to_consumer</th>\n",
       "      <th>Complaint_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10/01/2016</td>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Incorrect information on credit report</td>\n",
       "      <td>Account status</td>\n",
       "      <td>I have outdated information on my credit repor...</td>\n",
       "      <td>Company has responded to the consumer and the ...</td>\n",
       "      <td>TRANSUNION INTERMEDIATE HOLDINGS, INC.</td>\n",
       "      <td>AL</td>\n",
       "      <td>352XX</td>\n",
       "      <td>Web</td>\n",
       "      <td>10/05/2016</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>2141773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10/17/2016</td>\n",
       "      <td>Consumer Loan</td>\n",
       "      <td>Vehicle loan</td>\n",
       "      <td>Managing the loan or lease</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I purchased a new car on XXXX XXXX. The car de...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CITIZENS FINANCIAL GROUP, INC.</td>\n",
       "      <td>PA</td>\n",
       "      <td>177XX</td>\n",
       "      <td>Web</td>\n",
       "      <td>10/20/2016</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>2163100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Date_received           Product   Sub_product  \\\n",
       "0    10/01/2016  Credit reporting           NaN   \n",
       "1    10/17/2016     Consumer Loan  Vehicle loan   \n",
       "\n",
       "                                    Issue       Sub_issue  \\\n",
       "0  Incorrect information on credit report  Account status   \n",
       "1              Managing the loan or lease             NaN   \n",
       "\n",
       "                        Consumer_complaint_narrative  \\\n",
       "0  I have outdated information on my credit repor...   \n",
       "1  I purchased a new car on XXXX XXXX. The car de...   \n",
       "\n",
       "                             Company_public_response  \\\n",
       "0  Company has responded to the consumer and the ...   \n",
       "1                                                NaN   \n",
       "\n",
       "                                  Company State ZIP_code Submitted_via  \\\n",
       "0  TRANSUNION INTERMEDIATE HOLDINGS, INC.    AL    352XX           Web   \n",
       "1          CITIZENS FINANCIAL GROUP, INC.    PA    177XX           Web   \n",
       "\n",
       "  Date_sent_to_company Company_response_to_consumer  Complaint_ID  \n",
       "0           10/05/2016      Closed with explanation       2141773  \n",
       "1           10/20/2016      Closed with explanation       2163100  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre_processing, **Clean data**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. lovercase,\n",
    "2. Remove number, (just for sure),\n",
    "3. remove website,  (just for sure),\n",
    "4. remove xxxx and xx number, \n",
    "5. remove stop words and 'etc', \n",
    "6. Tokenize and Clean-up using re.sub, BeautifulSoup and nltk,\n",
    "7. Lemmatization by spacy.load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from farhad.credit_card.TextEditor import Remove_repetitive_words, Remove_stop_words, List_cleaner_complaint_root\n",
    "from farhad.credit_card.pre_cleaner import pre_clean_root\n",
    "from dask import delayed\n",
    "from farhad.Farhadcolor import tcolors,bcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[92m\n",
      "\u001b[0m\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 272657 entries, 0 to 272656\n",
      "Data columns (total 5 columns):\n",
      "Date_received    272657 non-null object\n",
      "Complaint_ID     272657 non-null int64\n",
      "new_product      272657 non-null object\n",
      "new_issue        272657 non-null object\n",
      "new              272657 non-null object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 10.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pre_clean_root(df_data)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[272657/272657][Remove repetitive words from new]*** Done! ***\n",
      "[272657/272657][Remove stop_words from new]*** Done! ***\n",
      "[159615/272657][Clean for complaint root]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "text1 = delayed(Remove_repetitive_words)(df['new'],'Remove repetitive words from new')\n",
    "text1 = delayed(Remove_stop_words)(text1,'Remove stop_words from new')\n",
    "text1 = delayed(List_cleaner_complaint_root)(text1,title='Clean for complaint root')\n",
    "df['new'] = text1.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('data/clean_creditcard_data2.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:70px;border: 4px solid #3590B7;background-color:#3550B7;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h1> gensim.models.Doc2Vec: <h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing , text Embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re \n",
    "import nltk\n",
    "from datetime import datetime\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn import utils\n",
    "from farhad.Farhadcolor import tcolors,bcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 272657 entries, 0 to 272656\n",
      "Data columns (total 5 columns):\n",
      "Date_received    272657 non-null object\n",
      "Complaint_ID     272657 non-null int64\n",
      "new_product      272657 non-null object\n",
      "new_issue        272657 non-null object\n",
      "new              272657 non-null object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 10.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df_data = pd.read_csv('data/clean_creditcard_data.csv')\n",
    "df_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tagged = df_data.apply(lambda r: TaggedDocument(words=tokenize_text(r['new']),tags=[r.Complaint_ID]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272657\n",
      "TaggedDocument(['credit', 'reporting', 'company', 'investigation', 'inadequate', 'help', 'phon'], [1420702])\n"
     ]
    }
   ],
   "source": [
    "print(len(data_tagged))\n",
    "print(data_tagged[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 272657/272657 [00:00<00:00, 2494457.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[44m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 272657/272657 [00:00<00:00, 2484215.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 49s, sys: 34.1 s, total: 4min 23s\n",
      "Wall time: 3min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(bcolors.BLUE)\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "model_dbow = Doc2Vec(dm=0, vector_size=50, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "model_dbow.build_vocab([x for x in tqdm(data_tagged.values)])\n",
    "\n",
    "model_dbow.train(utils.shuffle([x for x in tqdm(data_tagged.values)]), total_examples=len(data_tagged.values), epochs=20)\n",
    "model_dbow.alpha -= 0.002\n",
    "model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.4624121e-03,  7.3492685e-03, -9.2543373e-03, -3.6220197e-03,\n",
       "       -6.2302966e-03,  1.1246208e-03, -5.2438467e-03, -7.1793608e-03,\n",
       "       -1.3212202e-03,  2.4204529e-03,  2.2101337e-03, -2.5645162e-03,\n",
       "       -7.5509818e-04,  4.1392799e-03,  1.9190321e-03, -8.9954585e-03,\n",
       "       -6.1504859e-03,  9.3579730e-03, -9.6000293e-03,  9.4152074e-03,\n",
       "        7.2968923e-03,  9.9190902e-03,  4.3315049e-03, -9.5943045e-03,\n",
       "        7.0003476e-03, -8.7571451e-03, -5.6841723e-03,  6.3664159e-03,\n",
       "       -9.7313132e-03, -5.9392261e-03,  1.8470007e-03, -4.7866390e-03,\n",
       "        7.8961300e-03,  3.2332179e-03,  1.1529505e-03,  5.6947675e-03,\n",
       "        7.1803695e-03,  2.9428631e-05, -6.7274808e-03,  8.6073478e-04,\n",
       "       -4.3114908e-03,  9.0271840e-03,  1.1703101e-03,  9.4434163e-03,\n",
       "       -3.4905639e-03, -8.8853510e-03,  8.2035940e-03, -4.5004874e-03,\n",
       "       -3.6665569e-03,  4.0054307e-03], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dbow.infer_vector('reporting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    regressors = zip(*[(model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec = vec_for_learning(model_dbow, data_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[44m\u001b[37m\n",
      "example: <zip object at 0x13187e488>\n"
     ]
    }
   ],
   "source": [
    "print(bcolors.BLUE+tcolors.WHITE)\n",
    "#print('len text_vec:',len(text_vec))\n",
    "#print('len text_vec[2]',len(text_vec[3]))\n",
    "print('example:',text_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:70px;border: 4px solid #3590B7;background-color:#3550B7;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h1> sklearn: <h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #3550B7;background-color:#BFE6FF;color:black;border-radius: 5px;padding:7px\">\n",
    "  <strong> Refrence: </strong><br>\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from farhad.Farhadcolor import tcolors,bcolors\n",
    "import warnings \n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "#from sklearn.lda import LatentDirichletAllocation    # Linear Discriminant Analysis \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer # analyzer='word'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract_topic_words(model,n_top_words):\n",
    "    topic_words = {}\n",
    "\n",
    "    for topic, comp in enumerate(model.components):\n",
    "        # for the n-dimensional array \"arr\":\n",
    "        # argsort() returns a ranked n-dimensional array of arr, call it \"ranked_array\"\n",
    "        # which contains the indices that would sort arr in a descending fashion\n",
    "        # for the ith element in ranked_array, ranked_array[i] represents the index of the\n",
    "        # element in arr that should be at the ith index in ranked_array\n",
    "        # ex. arr = [3,7,1,0,3,6]\n",
    "        # np.argsort(arr) -> [3, 2, 0, 4, 5, 1]\n",
    "        # word_idx contains the indices in \"topic\" of the top num_top_words most relevant\n",
    "        # to a given topic ... it is sorted ascending to begin with and then reversed (desc. now)    \n",
    "        word_idx = np.argsort(comp)[::-1][:n_top_words]\n",
    "\n",
    "        # store the words most relevant to the topic\n",
    "        topic_words[topic] = [vocab[i] for i in word_idx]\n",
    "        \n",
    "    for topic, words in topic_words.items():\n",
    "        print(bcolors.Black,'               ',bcolors.ENDC)\n",
    "        print('Topic: %d' % topic)\n",
    "        print(\"  %s' % ', \".join(words)) \n",
    "        print(bcolors.Black,'               ',bcolors.ENDC)\n",
    "    return topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 272657 entries, 0 to 272656\n",
      "Data columns (total 5 columns):\n",
      "Date_received    272657 non-null object\n",
      "Complaint_ID     272657 non-null int64\n",
      "new_product      272657 non-null object\n",
      "new_issue        272657 non-null object\n",
      "new              272657 non-null object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 10.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df_data = pd.read_csv('data/clean_creditcard_data.csv')\n",
    "df_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 10\n",
      "iteration: 2 of max_iter: 10\n",
      "iteration: 3 of max_iter: 10\n",
      "iteration: 4 of max_iter: 10\n",
      "iteration: 5 of max_iter: 10\n",
      "iteration: 6 of max_iter: 10\n",
      "iteration: 7 of max_iter: 10\n",
      "iteration: 8 of max_iter: 10\n",
      "iteration: 9 of max_iter: 10\n",
      "iteration: 10 of max_iter: 10\n",
      "CPU times: user 5min 52s, sys: 2.67 s, total: 5min 55s\n",
      "Wall time: 6min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "number_of_classfication = 11\n",
    "\n",
    "sk_LDA = Pipeline([('vect', CountVectorizer(ngram_range=(1,2))),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('lda', LatentDirichletAllocation(n_components=number_of_classfication,\n",
    "                                                 learning_method='batch',learning_decay=0.2,\n",
    "                                                 batch_size=50,verbose=1,random_state=0)),\n",
    "              ])\n",
    "\n",
    "result_sk = sk_LDA.fit_transform(df_data['new'])\n",
    "label_sk = np.argmax(result_sk,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenght of data   : 272657\n",
      "number of class  : 11\n",
      "-----------------------------------------------------------------------------\n",
      "example of data  : [0.0170042  0.0170043  0.01700513 0.01700482 0.01700504 0.0170044\n",
      " 0.01700442 0.01700426 0.01700442 0.8299547  0.01700431]\n",
      "example of label : 9\n",
      "\n",
      "example of data  : [0.02036431 0.0203644  0.02036453 0.79634898 0.020365   0.02036653\n",
      " 0.02036835 0.02036433 0.02036437 0.02036486 0.02036435]\n",
      "example of label : 3\n"
     ]
    }
   ],
   "source": [
    "print('lenght of data   :',len(result_sk))\n",
    "print('number of class  :',len(result_sk[3]))\n",
    "print('-----------------------------------------------------------------------------')\n",
    "print('example of data  :',result_sk[4])\n",
    "print('example of label :',label_sk[4])\n",
    "print()\n",
    "print('example of data  :',result_sk[12])\n",
    "print('example of label :',label_sk[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Pipeline' object has no attribute 'components'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-aae41a83323e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtopic_workd_sk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExtract_topic_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msk_LDA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_top_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumber_of_classfication\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-6d86cea7d654>\u001b[0m in \u001b[0;36mExtract_topic_words\u001b[0;34m(model, n_top_words)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtopic_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;31m# for the n-dimensional array \"arr\":\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# argsort() returns a ranked n-dimensional array of arr, call it \"ranked_array\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Pipeline' object has no attribute 'components'"
     ]
    }
   ],
   "source": [
    "topic_workd_sk = Extract_topic_words(sk_LDA,n_top_words=number_of_classfication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:70px;border: 4px solid #3500B7;background-color:#3550B7;color:white;text-align:center;border-radius: 50%;padding:3px\">\n",
    "    <h1> Plotting words and documents in 2D with SVD: <h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import push_notebook, show, output_notebook\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import ColumnDataSource, LabelSet\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_model = Pipeline([('vect', CountVectorizer(ngram_range=(1,2))),\n",
    "               ('tfidf', TfidfTransformer())\n",
    "              ])\n",
    "data_vectorized = vectorized_model.fit_transform(df_data['new'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "getrow() missing 1 required positional argument: 'i'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-038bc99501d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_vectorized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: getrow() missing 1 required positional argument: 'i'"
     ]
    }
   ],
   "source": [
    "data_vectorized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=11)\n",
    "words_2d = svd.fit_transform(data_vectorized.T)\n",
    "\n",
    "df = pd.DataFrame(columns=['x', 'y', 'word'])\n",
    "df['x'], df['y'], df['word'] = words_2d[:,0], words_2d[:,1], vectorizer.get_feature_names()\n",
    "source = ColumnDataSource(ColumnDataSource.from_df(df))\n",
    "\n",
    "\n",
    "labels = LabelSet(x=\"x\", y=\"y\", text=\"word\", y_offset=8,\n",
    "                  text_font_size=\"8pt\", text_color=\"#555555\",\n",
    "                  source=source, text_align='center')\n",
    " \n",
    "plot = figure(plot_width=600, plot_height=600)\n",
    "plot.circle(\"x\", \"y\", size=12, source=source, line_color=\"black\", fill_alpha=0.8)\n",
    "plot.add_layout(labels)\n",
    "show(plot, notebook_handle=True)\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

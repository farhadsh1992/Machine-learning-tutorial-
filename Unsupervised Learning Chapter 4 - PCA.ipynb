{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Unsupervised Learning Chapter 4 : Principal Component Analysis (PCA) </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Refrance:</h2>\n",
    "https://medium.com/@raghavan99o/principal-component-analysis-pca-explained-and-implemented-eeab7cb73b72\n",
    "\n",
    "https://medium.com/100-days-of-algorithms/day-92-pca-bdb66840a8fb\n",
    "\n",
    "https://medium.com/journey-2-artificial-intelligence/unraveling-pca-principal-component-analysis-in-python-d23b081409cf\n",
    "\n",
    "\n",
    "<b>Book</b><br>\n",
    "machin learning.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is a statistical technique which reduces the dimensions  n(> 10) of the data and help us understand,<br>\n",
    "Principal components are basically vectors that are linearly uncorrelated and have a variance with in data.<br>\n",
    "From the principal components top p is picked which have the most variance.<br>\n",
    "Our goal separate data by drawing a line (or a plane) between data. If we find out the dimension which has maximum variance, then it solves part of the problem, now all we have to use suitable algorithm to draw the line or plane which splits the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Theory:</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of PCA is to perform a linear. projection of the data onto a lower-dimensional subspace where the variance is maximized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s say we have a bunch of multi-dimensional data stored in a matrix X. Each row represents a sample, and each column represents a variable.<br>\n",
    "\n",
    "We say that two variables are correlated if there is a linear relationship in between them.<br>Their scatterplot may look similarly to this one:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*VUuO9kIhXOyhLD6gE6dyGQ.png\"></img>\n",
    "https://cdn-images-1.medium.com/max/1600/1*VUuO9kIhXOyhLD6gE6dyGQ.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, the variables on the next scatterplot are uncorrelated.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the first plot seems to be more useful, PCA takes the advantage of the second one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>implement PCA</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets begin by generating random data with 3 dimensions with 40 samples . We will have two class with 20 samples per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "vec1 = np.array([0, 0, 0])\n",
    "mat1 = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "sample_for_class1 = np.random.multivariate_normal(vec1, mat1, 20).T\n",
    "assert sample_for_class1.shape == (3, 20), \"The dimension of the sample_for_class1 matrix is not 3x20\"\n",
    "\n",
    "vec2 = np.array([1, 1, 1])\n",
    "mat2 = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "sample_for_class2 = np.random.multivariate_normal(vec2, mat2, 20).T\n",
    "assert sample_for_class2.shape == (3, 20), \"The dimension of the sample_for_class2 matrix is not 3x20\"\n",
    "\n",
    "all_data = np.concatenate((sample_for_class1, sample_for_class2), axis=1)\n",
    "assert all_data.shape == (3, 40), \"The dimension of the all_data matrix is not 3x20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mean Vector:\n",
      " [[0.41667492]\n",
      " [0.69848315]\n",
      " [0.49242335]]\n",
      "The Scatter Matrix is :\n",
      " [[38.4878051  10.50787213 11.13746016]\n",
      " [10.50787213 36.23651274 11.96598642]\n",
      " [11.13746016 11.96598642 49.73596619]]\n"
     ]
    }
   ],
   "source": [
    "mean_dim1 = np.mean(all_data[0, :])\n",
    "mean_dim2 = np.mean(all_data[1, :])\n",
    "mean_dim3 = np.mean(all_data[2, :])\n",
    "\n",
    "mean_vector = np.array([[mean_dim1], [mean_dim2], [mean_dim3]])\n",
    "\n",
    "print('The Mean Vector:\\n', mean_vector)\n",
    "\n",
    "scatter_matrix = np.zeros((3,3))\n",
    "for i in range(all_data.shape[1]):\n",
    "    scatter_matrix += (all_data[:, i].reshape(3, 1) - mean_vector).dot((all_data[:, i].reshape(3, 1) - mean_vector).T)\n",
    "print('The Scatter Matrix is :\\n', scatter_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know how each of variables are related to each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the numpy library to compute the Eigenvectors and eigenvalues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_val, eig_vec = np.linalg.eig(scatter_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default numpy or any stats library gives out eigenvectors of unit length. Lets verify it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ev in eig_vec:\n",
    "    np.testing.assert_array_almost_equal(1.0, np.linalg.norm(ev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_val_sc, eig_vec_sc =eig_val, eig_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.16936779078195\n",
      "32.69471296321799\n",
      "26.596203282097097\n"
     ]
    }
   ],
   "source": [
    "# We Make a list of tuple containing (eigenvalue, eigenvector)\n",
    "eig_pairs = [(np.abs(eig_val_sc[i]), eig_vec_sc[:,i]) for i in range(len(eig_val_sc))]\n",
    "\n",
    "# We then Sort list of tuples by the eigenvalue\n",
    "eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# verify that the list is correctly sorted by decreasing eigenvalues\n",
    "for i in eig_pairs:\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we choose k the largest eigenvectors :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix W:\n",
      " [[-0.49210223 -0.64670286]\n",
      " [-0.47927902 -0.35756937]\n",
      " [-0.72672348  0.67373552]]\n"
     ]
    }
   ],
   "source": [
    "matrix_w = np.hstack((eig_pairs[0][1].reshape(3,1), eig_pairs[1][1].reshape(3,1)))\n",
    "print('Matrix W:\\n', matrix_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-dbada0efc765>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatrix_w\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mtransformed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"The matrix is not 2x40 dimensional.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_samples' is not defined"
     ]
    }
   ],
   "source": [
    "transformed = matrix_w.T.dot(all_samples)\n",
    "assert transformed.shape == (2,40), \"The matrix is not 2x40 dimensional.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*QMqMeGy6sp-FSlZ0RSQWpg.png\"></img>\n",
    "https://cdn-images-1.medium.com/max/1600/1*QMqMeGy6sp-FSlZ0RSQWpg.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

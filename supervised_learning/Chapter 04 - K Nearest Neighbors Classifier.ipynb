{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Chapter 4: K Nearest Neighbors Classifier</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Refrance:</h2>\n",
    "<b>Theory and coding:</b><br>\n",
    "https://medium.com/machine-learning-101/k-nearest-neighbors-classifier-1c1ff404d265\n",
    "<br><b>gitHub:</b><br>\n",
    "\n",
    "<br><b>orginal website:</b><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>How do we choose neighbors?</h3><br>\n",
    "<h3>1. Brute Force:</h3>\n",
    "Lets consider for simple case with two dimension plot. If we look mathematically, the simple intuition is to calculate the euclidean distance from point of interest ( of whose class we need to determine) to all the points in training set. Then we take class with majority points. This is called brute force method.<br>\n",
    "For N samples in D dimensions the running time complexity turns out to be O[DN²].  If we have small number of dimensions and training set, this would run in reasonable time. But as the training set size increases, the running time grows quickly.\n",
    "<b> Brute force performs worst when there are large dimensions and large training set.</b>\n",
    "\n",
    "<h3> 2. K-D Tree:</h3>\n",
    "To improve the running time, alternate approaches were invented on the line of building a growing tree from point of interest. These methods attempt to reduce the required number of distance calculations by efficiently encoding aggregate distance information for the sample.\n",
    ">(The basic idea is that if point A is very distant from point B, and point B is very close to point C, then we know that points A and C are very distant, without having to explicitly calculate their distance. )(This may not be correct all the time.)\n",
    "\n",
    "\n",
    "In this way, the computational cost of a nearest neighbors search can be reduced to O[D N *log(N)] or better. This is a significant improvement over brute-force for large N.\n",
    "<br><b>K-D tree performs well enough when D <20. With larger D it again takes longer time. This is known as “curse of dimensionality”.</b>\n",
    "    \n",
    "\n",
    "<h3>3. Ball Tree </h3>\n",
    "Ball tree assumes the data in multidimensional dimensional space and creates the nested hyper spheres. Query Time complexity is O[Dlog(N)]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Which one to choose when?</b><br>\n",
    "\n",
    "The selection of the relevant algorithm for problem at hand depends on the number of dimensions and the size of training set:\n",
    "1. For small sample size and small dimensions, brute force performs well.\n",
    "2. Sparsity of data : If data is sparse with small dimensions (< 20) KD tree will perform better than Ball Tree algorithm.\n",
    "3. Value of K (neighbors) : As the K increases, query time of both KD tree and Ball tree increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Sklearn K nearest and parameters</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(features_matrix, labels)\n",
    "predicted_values = neigh.predict(test_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Some important Parameters</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>n_neighbors :</b> Number of neighbors to consider. Default is 5.<br>\n",
    "<b>algorithm :</b> By default it is set to auto. Algorithm can be {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}.\n",
    "\n",
    ">Currently, algorithm = ‘auto’ selects ‘kd_tree’ if k < N/2 and the ‘effective_metric_’ is in the ‘VALID_METRICS’ list of ‘kd_tree’. It selects ‘ball_tree’ if k < N/2 and the ‘effective_metric_’ is not in the ‘VALID_METRICS’ list of ‘kd_tree’. It selects ‘brute’ if k >= N/2. This choice is based on the assumption that the number of query points is at least the same order as the number of training points, and that leaf_size is close to its default value of 30. [ reference : Sklearn Documentation].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Final Thoughts</b>\n",
    "Except brute force, K Nearest Neighbors implements tree like data structure to determine the distances from point of interest to points in training set. The selection of best algorithm depends on sparsity of data, number of neighbors requested and the dimension/number of features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
